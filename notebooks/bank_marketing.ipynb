{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Bank Telemarketing Analysis\n", "\n", "The dataset was collected from a telemarketing campaign by a Portuguese banking institution. \n", "\n", "Occasionally, customers were contacted more than once in order to attempt to sell term deposit subscriptions.\n", "\n", "Note: this assignment requires creating matplotlib figures using both matplotlib and seaborn. Unfortunately matplotlib can be a bit capricious when it comes to testing figures, so in order to make sure your figures are properly associated to the right variable you need to make sure you add a `plt.figure()` statement before calling any command that generates a figure. We have added it to the right places below, so it should be ready to go."]}, {"cell_type": "markdown", "metadata": {}, "source": ["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n", "\n", "KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n", "\n", "* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n", "* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n", "\n", "You will find instructions below about how to define each variable.\n", "\n", "Once you're happy with your code, upload your notebook to KATE to check your feedback."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Import The Dataset\n", "\n", "Let's first import the csv dataset, which you can download from the UCI machine learning datasets webpage, where this dataset is publicly available. \n", "\n", "The dataset can be downloaded from [here](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the following table we summarize what each variable in the dataset represents:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "| Variable  | Meaning | Type |\n", "| ------------- | ------------- | ------------- |\n", "| age  | Age of the client  | Numeric |\n", "| job  | Type of Job  | Categorical |\n", "| marital | Marital status | Categorical |\n", "| education | Type of Education | Categorical | \n", "| default | Has credit in default? | Binary |\n", "| balance | Average yearly balance in euros | Binary |\n", "| housing | Has housing loan | Binary |\n", "| contact | Contact communication type | Categorical |\n", "| day | Last contact day of the month | Numeric | \n", "| month | Last contact month of the year | Categorical |\n", "|duration | Last contact duration in seconds | Numeric |\n", "| campaign | Number of contact performed during this campaign and for this client | Numeric |\n", "|pdays | Number of days that passed by after the client was last contacted from a previous campaign | Numeric -1 means client was not previously contacted|\n", "|previous | Number of contacts performed before this campaign and for this client | Numeric |\n", "|poutcome | Outcome of the previous marketing campaign | Categorical |\n", "|y | This is the output variable. Has the client subscribed a term deposit? | Binary |\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To get started, let's load the bank dataset into a DataFrame using the `pandas` library.\n", "\n", "The dataset, `bank-full.csv` can be found in the `data` folder. Let's load it in and assign it to a variable called `df`.\n", "\n", "Before we do that, however, it it worth noting that this is not your standard `.csv` file! The fields are not separated with commas, but with semi-colons. Thankfully, `pandas` is smart enough to deal with this, we just need to specify the separator in the `sep` argument (here `sep=\";\"`).\n", "\n", "This is something to be aware of when working with new data files - you can't always assume their structure!"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"data/bank-full.csv\", sep=\";\")\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Checking for NaNs\n", "\n", "Let's being by getting familiar with our data.\n", "\n", "When you start working with a new dataset, it is vital to understand it! That means not only the obvious things, like how many rows (observations) and columns (features) you have, but also things like null values and potential errors (e.g. typos in strings or decimal places in numerical columns).\n", "\n", "One of the great things about `pandas` is that it has some really helpful methods to quickly get a snapshot of your data.\n", "\n", "One of these methods, `.info()` provides a great overview of your data, including:\n", "  - The number of rows\n", "  - The number of columns\n", "  - The data type of each column\n", "  - The number of non-null values\n", "  - The size in memory of your dataset\n", "  \n", "In the cell below, run `.info()` on the dataset and get to grips with the data!"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# Your code here:\n", "#\u00a0Extract info from the dataframe\n", "df.info()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**1. Do any columns in the dataset contain null values?**\n", "\n", "We can already tell from the output of `.info()` that the dataset does not contain any null values (which is great!).\n", "\n", "<br>\n", "\n", "However, there are other neat ways to check for null values that you should be aware of. For instance, the `.isna()` method will convert your DataFrame of values (numerical, string, and otherwise) into a DataFrame of boolean values (`True` or `False`), depending on whether or not each cell is null.\n", "\n", "<br>\n", "\n", "This is a bit much information to easily digest (since our DataFrame contains over $45,000$ rows). Thankfully, we can \"chain\" another `pandas` method right after `.isna()` to convert this DataFrame into a Series representing a column by column check for null values. In other words, we can use `.any()` to return a Series specifying whether or not each column contains any null values.\n", "\n", "<br>\n", "\n", "In the following cell, use `.isna()` and `.any()` to return a Series informing us whether or not each column contains any null values. Assign this Series to a variable called `df_nas_check_per_column`.\n", "\n", "<br>\n", "\n", "Note that we already know the answer (none of them contain null values) - this is just some good practice for working with `pandas`!"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "df_nas_check_per_column = df.isna().any()\n", "print(df_nas_check_per_column)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**2. Let's now check in one line if the whole dataset contains missing values!**\n", "\n", "In the previous question, we used `isna()` chained together with `.any()` to identify whether or not any column contained null values:\n", "  - `.isna()` told us whether or not any cell was null\n", "  - `.any()` aggregated the DataFrame of boolean values over the rows to give us a column representation of null values\n", "\n", "<br>\n", "\n", "We can actually use `.any()` again to aggregate over these columns to give us a single boolean value for whether or not there are any null values in the DataFrame. Once again, we obviously know the answer to this from inspecting `.info()` but it is still useful to know what tools and tricks `pandas` has to offer.\n", "\n", "<br>\n", "\n", "In the following cell, apply `.isna()` followed by `.any()` followed by `.any()` again to `df` to return a boolean value for whether or not the DataFrame contains any null values. Assign this to a variable called `df_nas_check_overall`"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "df_nas_check_overall = df.isna().any().any()\n", "print(df_nas_check_overall)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Dataset Exploration\n", "\n", "Now let's try some techniques to look at the data and discover some statistical properties of the attributes we are working with.\n", "\n", "<br>\n", "\n", "\n", "**3. Make a count plot showing the distribution of `yes` and `no` (that is, our target variable `y`).**\n", "\n", "The distribution of the classes in your training data is an important consideration when building a machine learning model. \n", "\n", "Consider an extreme case where 95% of your data points belong to one class, and only 5% belong to the other. \n", "\n", "In this case, a machine learning algorithm may learn to predict the majority class label for all datapoint, leading to an accuracy score of 95%. But is this a good model? \n", "\n", "Not at all! Let's now inspect the distribution of class labels in our data set. \n", "\n", "*Hint: You can use the `sns.countplot()` function to display the distribution of our classes.*"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["# We create a new figure to make sure other figures in the notebook don't get modified\n", "plt.figure()\n", "\n", "# Your code here\n", "count_plot = sns.countplot(data=df, x='y')\n", "plt.title('Distribution of Target Variable (Yes/No)')\n", "plt.xlabel('Class')\n", "plt.ylabel('count')\n", "plt.show()\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**4. Make the same count, but this time segment the data by the variable marital status. What can you observe?**\n", "\n", "*Hint: use the `hue` parameter in calling `sns.countplot()`*"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# We create a new figure to make sure other figures in the notebook don't get modified\n", "plt.figure()\n", "\n", "# Your code here\n", "count_by_marital_status = sns.countplot(data=df, x='y', hue='marital')\n", "\n", "plt.title('Subscription Distribution by Marital Status')\n", "plt.xlabel('Subscription Status')\n", "plt.ylabel('Count')\n", "plt.legend(title='Marital Status')\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**5. Using `pandas`, create a boxplot of the balance variable. What can you conclude from the plot?**\n", "\n", "*Hint: use the `.boxplot()` method from `pandas`*"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["# We create a new figure to make sure other figures in the notebook don't get modified\n", "plt.figure()\n", "\n", "# Your code here\n", "balance_boxplot = df.boxplot(column='balance')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**6. Plot the boxplot of the `balance` variable, grouped by the output variable `y`. What can you say concerning the distribution of the variable between the yes and no groups?**\n", "\n", "Note that the `y` variable should be along the x-axis, and `balance` should be on the y-axis.\n", "\n", "*Hint: use the `sns.boxplot()` function but specify both `x` and `y`*"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["# We create a new figure to make sure other figures in the notebook don't get modified\n", "plt.figure()\n", "\n", "# Your code here:\n", "balance_boxplot_grouped_by_y = sns.boxplot(data=df, y='balance', x='y')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Dataset Preprocessing \n", "\n", "<br>\n", "\n", "**7. The `duration` column is in seconds. Let's convert the units to be in minutes.**\n", "\n", "Create a copy of the dataframe called `df_new` using the `.copy()` function. Do this to avoid overwriting the original one.\n", "\n", "\n", "Convert the `duration` column to be in minutes, and save it in the duration column "]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "df_new = df.copy()\n", "df_new['duration'] = df_new['duration'] / 60\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**8. Convert the `job`, `education`, and `y` variables into suitable types for training a machine learning model**\n", "\n", "First, create a copy of `df_new` called `df_numerical`. This will avoid overwriting the original DataFrame.\n", "\n", "\n", "Next, to convert the categorical variables into a binary vector representation you can use the `pandas` function `get_dummies()`. This function can convert a numerical column into a categorical one. The function considers the possible categorical values that your feature can assume, and creates a vector of the length of the possible values, with 1 or 0 depending if that particular value is present or not. \n", "\n", "\n", "The `get_dummies()` function also accepts a `prefix` argument which specifies the prefix that will be added to the new variables. In this case, specify `job` for the job feature, and `education` for the education feature.\n", "\n", "\n", "The `y` column initially contains `no` and `yes` values. These need to be converted into a binary representation (i.e. `0` and `1`). Overwrite the `y` column in `df_numerical` with the output of `get_dummies()`. Note, however, that this method will create two columns, which is redundant: to prevent this, pass `drop_first=True` to `get_dummies()`. \n", "\n", "Finally, concatenate the new variables to the `df_numerical` dataframe. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "df_numerical = df_new.copy()\n", "job_dummies = pd.get_dummies(df_numerical['job'], prefix='job')\n", "education_dummies = pd.get_dummies(df_numerical['education'], prefix='education')\n", "\n", "y_binary = pd.get_dummies(df_numerical['y'], drop_first=True)\n", "\n", "df_numerical['y'] = y_binary\n", "\n", "df_numerical = pd.concat([df_numerical, job_dummies, education_dummies], axis=1)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**9. Make a new dataset with only `education_primary`, `education_secondary`, `education_tertiary`, `education_unknown`, `balance`, `duration`, `campaign`, `pdays`, `previous` and `y`.**\n", "\n", "Create a new dataset from `df_numerical`, retaining only the columns `education_primary`, `education_secondary`, `education_tertiary`, `education_unknown`, `balance`, `duration`, `campaign`, `pdays`, `previous` and `y`. \n", "\n", "Save it in a new dataframe named `df_numerical_education`.\n", "\n", "Remember to also use the `.copy()` method!"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "selected_columns = [  \n", "    'education_primary',\n", "    'education_secondary',\n", "    'education_tertiary',\n", "    'education_unknown',\n", "    'balance',\n", "    'duration',\n", "    'campaign',\n", "    'pdays',\n", "    'previous',\n", "    'y'\n", "]\n", "\n", "df_numerical_education = df_numerical[selected_columns].copy()\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**10. Normalize the `df_numerical_education` dataset using min-max normalisation, and save it as a new variable called `df_scaled`.**\n", "\n", "Given the dataset `df_numerical_education`, scale the values between the minimum and the maximum of the dataset, using.\n", "\n", "The min max scaling formula (considering you want to scale between 0 and 1) is as follows: \n", "\n", "```Python\n", "df_scaled=(originalDF - min_val_originalDF) / (max_val_originalDF - min_val_originalDF) \n", "```"]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "# Select only numeric columns for scaling\n", "numeric_cols = df_numerical_education.select_dtypes(include=['number']).columns\n", "\n", "# Apply min-max normalization\n", "df_scaled = df_numerical_education[numeric_cols].copy()\n", "df_scaled = (df_scaled - df_scaled.min()) / (df_scaled.max() - df_scaled.min())\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Fitting a logistic regression model "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**11. Divide the dataset into training and testing sets.**\n", "\n", "Use the `train_test_split` from the `sklearn.model_selection` package to split the `df_scaled` dataframe into $4$ subsets: \n", " - `X_train`\n", " - `X_test`\n", " - `y_train`\n", " - `y_test`\n", "\n", "In doing so, specify the text size to be `0.3` and set the `random_state=101` for reproducibility."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "#\u00a0X_train, X_test, y_train, y_test = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**12. Create a Logistic Regression model and fit it on the training data**\n", "\n", "Assign your model to a variable called `logmodel`\n", "\n", "*Hint: use the `.fit()` method*"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# Your code here\n", "#\u00a0logmodel = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**13. Use the `.predict()` method from your model to generate predictions on `X_test`**\n", "\n", "Assign the output to a variable called `predictions`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Your code here\n", "# predictions = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Once you have generated these predictions, uncomment the cell below to check out the accuracy score of your model. Think about what this metrics [means](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). Is this a good model? Is `accuracy` a good metric?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# from sklearn.metrics import accuracy_score\n", "# accuracy_score(y_test, predictions)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.13"}}, "nbformat": 4, "nbformat_minor": 2}